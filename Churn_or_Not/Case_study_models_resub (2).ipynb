{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "# baseline in performance with logistic regression model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "##https://machinelearningmastery.com/autoencoder-for-classification/\n",
    "# train autoencoder for classification with no compression in the bottleneck layer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot\n",
    "from keras.models import load_model \n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>Partner_No</th>\n",
       "      <th>Partner_Yes</th>\n",
       "      <th>Dependents_No</th>\n",
       "      <th>...</th>\n",
       "      <th>PaymentMethod_Bank transfer (automatic)</th>\n",
       "      <th>PaymentMethod_Credit card (automatic)</th>\n",
       "      <th>PaymentMethod_Electronic check</th>\n",
       "      <th>PaymentMethod_Mailed check</th>\n",
       "      <th>mnth_tenure_group2_1 - 12</th>\n",
       "      <th>mnth_tenure_group2_13 - 24</th>\n",
       "      <th>mnth_tenure_group2_25 - 36</th>\n",
       "      <th>mnth_tenure_group2_37 - 48</th>\n",
       "      <th>mnth_tenure_group2_49 - 60</th>\n",
       "      <th>mnth_tenure_group2_61 - 72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SeniorCitizen  MonthlyCharges  TotalCharges  Churn  \\\n",
       "0           0              0           29.85         29.85      0   \n",
       "1           1              0           56.95       1889.50      0   \n",
       "2           2              0           53.85        108.15      1   \n",
       "3           3              0           42.30       1840.75      0   \n",
       "4           4              0           70.70        151.65      1   \n",
       "\n",
       "   gender_Female  gender_Male  Partner_No  Partner_Yes  Dependents_No  ...  \\\n",
       "0              1            0           0            1              1  ...   \n",
       "1              0            1           1            0              1  ...   \n",
       "2              0            1           1            0              1  ...   \n",
       "3              0            1           1            0              1  ...   \n",
       "4              1            0           1            0              1  ...   \n",
       "\n",
       "   PaymentMethod_Bank transfer (automatic)  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        1   \n",
       "4                                        0   \n",
       "\n",
       "   PaymentMethod_Credit card (automatic)  PaymentMethod_Electronic check  \\\n",
       "0                                      0                               1   \n",
       "1                                      0                               0   \n",
       "2                                      0                               0   \n",
       "3                                      0                               0   \n",
       "4                                      0                               1   \n",
       "\n",
       "   PaymentMethod_Mailed check  mnth_tenure_group2_1 - 12  \\\n",
       "0                           0                          1   \n",
       "1                           1                          0   \n",
       "2                           1                          1   \n",
       "3                           0                          0   \n",
       "4                           0                          1   \n",
       "\n",
       "   mnth_tenure_group2_13 - 24  mnth_tenure_group2_25 - 36  \\\n",
       "0                           0                           0   \n",
       "1                           0                           1   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   mnth_tenure_group2_37 - 48  mnth_tenure_group2_49 - 60  \\\n",
       "0                           0                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           1                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   mnth_tenure_group2_61 - 72  \n",
       "0                           0  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"E:/Venu/Applied ai course/Assignments data and ipynb/Case study 1/tel_churn.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7032, 52)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>Partner_No</th>\n",
       "      <th>Partner_Yes</th>\n",
       "      <th>Dependents_No</th>\n",
       "      <th>Dependents_Yes</th>\n",
       "      <th>PhoneService_No</th>\n",
       "      <th>...</th>\n",
       "      <th>PaymentMethod_Bank transfer (automatic)</th>\n",
       "      <th>PaymentMethod_Credit card (automatic)</th>\n",
       "      <th>PaymentMethod_Electronic check</th>\n",
       "      <th>PaymentMethod_Mailed check</th>\n",
       "      <th>mnth_tenure_group2_1 - 12</th>\n",
       "      <th>mnth_tenure_group2_13 - 24</th>\n",
       "      <th>mnth_tenure_group2_25 - 36</th>\n",
       "      <th>mnth_tenure_group2_37 - 48</th>\n",
       "      <th>mnth_tenure_group2_49 - 60</th>\n",
       "      <th>mnth_tenure_group2_61 - 72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeniorCitizen  MonthlyCharges  TotalCharges  gender_Female  gender_Male  \\\n",
       "0              0           29.85         29.85              1            0   \n",
       "1              0           56.95       1889.50              0            1   \n",
       "2              0           53.85        108.15              0            1   \n",
       "3              0           42.30       1840.75              0            1   \n",
       "4              0           70.70        151.65              1            0   \n",
       "\n",
       "   Partner_No  Partner_Yes  Dependents_No  Dependents_Yes  PhoneService_No  \\\n",
       "0           0            1              1               0                1   \n",
       "1           1            0              1               0                0   \n",
       "2           1            0              1               0                0   \n",
       "3           1            0              1               0                1   \n",
       "4           1            0              1               0                0   \n",
       "\n",
       "   ...  PaymentMethod_Bank transfer (automatic)  \\\n",
       "0  ...                                        0   \n",
       "1  ...                                        0   \n",
       "2  ...                                        0   \n",
       "3  ...                                        1   \n",
       "4  ...                                        0   \n",
       "\n",
       "   PaymentMethod_Credit card (automatic)  PaymentMethod_Electronic check  \\\n",
       "0                                      0                               1   \n",
       "1                                      0                               0   \n",
       "2                                      0                               0   \n",
       "3                                      0                               0   \n",
       "4                                      0                               1   \n",
       "\n",
       "   PaymentMethod_Mailed check  mnth_tenure_group2_1 - 12  \\\n",
       "0                           0                          1   \n",
       "1                           1                          0   \n",
       "2                           1                          1   \n",
       "3                           0                          0   \n",
       "4                           0                          1   \n",
       "\n",
       "   mnth_tenure_group2_13 - 24  mnth_tenure_group2_25 - 36  \\\n",
       "0                           0                           0   \n",
       "1                           0                           1   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   mnth_tenure_group2_37 - 48  mnth_tenure_group2_49 - 60  \\\n",
       "0                           0                           0   \n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           1                           0   \n",
       "4                           0                           0   \n",
       "\n",
       "   mnth_tenure_group2_61 - 72  \n",
       "0                           0  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here dropping target variable for splitting input and output data for applying model\n",
    "input=data.drop('Churn',axis=1)\n",
    "input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7032,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=data['Churn']\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying models without autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1,x_test,y_train1,y_test=train_test_split(input, target,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5625, 50)\n",
      "(1407, 50)\n",
      "(5625,)\n",
      "(1407,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train1.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train1.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTEENN()\n",
    "x_train, y_train = sm.fit_sample(x_train1,y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4654, 50)\n",
      "(1407, 50)\n",
      "(4654,)\n",
      "(1407,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #Logistic Regression with SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, loss='log', random_state=24)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lg = SGDClassifier(alpha=0.001,loss='log',penalty='l2',random_state=24)\n",
    "model_lg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6076759061833689\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.57      0.68      1015\n",
      "           1       0.39      0.72      0.50       392\n",
      "\n",
      "    accuracy                           0.61      1407\n",
      "   macro avg       0.61      0.64      0.59      1407\n",
      "weighted avg       0.71      0.61      0.63      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = model_lg.predict(x_test)\n",
    "score = model_lg.score(x_test, y_test)\n",
    "print(score)\n",
    "print(metrics.classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[574 441]\n",
      " [111 281]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.589891223994086"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desicion tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=50 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=5 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=50 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "                   param_distributions={'max_depth': [1, 5, 10, 50],\n",
       "                                        'min_samples_split': [5, 10, 100, 500]},\n",
       "                   return_train_score=True, verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT = DecisionTreeClassifier()\n",
    "max_depth=[1,5,10,50]\n",
    "min_samples_split=[5,10,100,500]\n",
    "parameters = {'max_depth':max_depth,'min_samples_split':min_samples_split}\n",
    "clf_dt = RandomizedSearchCV(DT, parameters, cv= 10,return_train_score=True,verbose=2)\n",
    "clf_dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=10, min_samples_split=5)\n"
     ]
    }
   ],
   "source": [
    "print(clf_dt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, min_samples_split=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_best=DecisionTreeClassifier(max_depth=10, min_samples_split=10)\n",
    "clf_best.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7121535181236673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.79      1015\n",
      "           1       0.49      0.67      0.56       392\n",
      "\n",
      "    accuracy                           0.71      1407\n",
      "   macro avg       0.67      0.70      0.67      1407\n",
      "weighted avg       0.75      0.71      0.72      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_dt = clf_best.predict(x_test)\n",
    "score_dt = clf_best.score(x_test, y_test)\n",
    "print(score_dt)\n",
    "print(metrics.classification_report(y_test, y_predict_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[740 275]\n",
      " [130 262]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6745966256749079"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_predict_dt, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=RandomForestClassifier(n_jobs=-1, random_state=25),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={'max_depth': [3, 5, 10],\n",
       "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000015AF0336880>,\n",
       "                                        'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000015AF0341E80>,\n",
       "                                        'n_estimators': [100, 200, 500, 1000,\n",
       "                                                         2000]},\n",
       "                   random_state=25, return_train_score=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "param_dist = {'n_estimators':[100,200,500,1000,2000],\n",
    "              'max_depth':[3,5,10],\n",
    "              \"min_samples_split\": sp_randint(10,19),\n",
    "              \"min_samples_leaf\": sp_randint(25,65)}\n",
    "\n",
    "clf = RandomForestClassifier(random_state=25,n_jobs=-1)\n",
    "\n",
    "rf_random = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=5,cv=5,random_state=25,return_train_score=True)\n",
    "\n",
    "rf_random.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=10, min_samples_leaf=33, min_samples_split=14,\n",
      "                       n_estimators=200, n_jobs=-1, random_state=25)\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=33, min_samples_split=14,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
    "            oob_score=False, random_state=25, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_leaf=33, min_samples_split=14,\n",
       "                       n_estimators=200, n_jobs=-1, random_state=25)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7263681592039801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.71      0.79      1015\n",
      "           1       0.51      0.77      0.61       392\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.70      0.74      0.70      1407\n",
      "weighted avg       0.78      0.73      0.74      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_rf = rf_random.predict(x_test)\n",
    "score_rf = rf_random.score(x_test, y_test)\n",
    "print(score_rf)\n",
    "print(metrics.classification_report(y_test, y_predict_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[719 296]\n",
      " [ 89 303]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   46.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:14:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100,...\n",
       "                                           reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.1, 0.3, 0.5, 1],\n",
       "                                        'learning_rate': [0.01, 0.03, 0.05, 0.1,\n",
       "                                                          0.15, 0.2],\n",
       "                                        'max_depth': [3, 5, 10],\n",
       "                                        'min_child_weight': [1, 4, 7],\n",
       "                                        'n_estimators': [100, 200, 500, 1000,\n",
       "                                                         2000],\n",
       "                                        'subsample': [0.1, 0.3, 0.5, 1]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "model_xgb=XGBClassifier()\n",
    "\n",
    "prams={\n",
    "    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n",
    "     'n_estimators':[100,200,500,1000,2000],\n",
    "     'max_depth':[3,5,10],\n",
    "    'min_child_weight':[1,4,7],\n",
    "    'colsample_bytree':[0.1,0.3,0.5,1],\n",
    "    'subsample':[0.1,0.3,0.5,1]\n",
    "}\n",
    "model_xgb=RandomizedSearchCV(model_xgb,param_distributions=prams,verbose=10,n_jobs=-1,cv=5)\n",
    "model_xgb.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.5, 'n_estimators': 2000, 'min_child_weight': 1, 'max_depth': 10, 'learning_rate': 0.05, 'colsample_bytree': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print(model_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:14:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=200, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb =XGBClassifier(subsample=1,n_estimators=200,min_child_weight=1,max_depth=5,learning_rate=0.1,colsample_bytree=0.5)\n",
    "model_xgb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7448471926083866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.75      0.81      1015\n",
      "           1       0.53      0.72      0.61       392\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.70      0.74      0.71      1407\n",
      "weighted avg       0.78      0.74      0.75      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_xgb = model_xgb.predict(x_test)\n",
    "score_xgb = model_xgb.score(x_test, y_test)\n",
    "print(score_xgb)\n",
    "print(metrics.classification_report(y_test, y_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[764 251]\n",
      " [108 284]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7112400807432218"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_predict_xgb, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  45 | elapsed:   20.8s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   24.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=AdaBoostClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.01, 0.03, 0.05],\n",
       "                                        'n_estimators': [100, 200, 500]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adaboost=AdaBoostClassifier()\n",
    "\n",
    "parameters={\n",
    "    'learning_rate':[0.01,0.03,0.05],\n",
    "     'n_estimators':[100,200,500]\n",
    "}\n",
    "model_adaboost=RandomizedSearchCV(model_adaboost,param_distributions=parameters,verbose=10,n_jobs=-1,cv=5)\n",
    "model_adaboost.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'learning_rate': 0.05}\n"
     ]
    }
   ],
   "source": [
    "print(model_adaboost.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.05, n_estimators=500)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adaboost=AdaBoostClassifier(learning_rate=0.05,n_estimators=500)\n",
    "model_adaboost.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728500355366027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.70      0.79      1015\n",
      "           1       0.51      0.80      0.62       392\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.70      0.75      0.70      1407\n",
      "weighted avg       0.79      0.73      0.74      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_ada = model_adaboost.predict(x_test)\n",
    "score_ada= model_adaboost.score(x_test, y_test)\n",
    "print(score_ada)\n",
    "print(metrics.classification_report(y_test, y_predict_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[712 303]\n",
      " [ 79 313]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7047572905131044"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_predict_ada, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparision between various Classification models\n",
      "+--------------------------+----------+\n",
      "|     Classifier Model     | F1-score |\n",
      "+--------------------------+----------+\n",
      "|   Logistic Regression    |    58    |\n",
      "| Decision Tree Classifier |    70    |\n",
      "| Random Forest Classifier |    72    |\n",
      "|    Xgboost Classifier    |    73    |\n",
      "|   Adaboost Classifier    |    73    |\n",
      "+--------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "print('Comparision between various Classification models')\n",
    "X= PrettyTable([\"Classifier Model\",\"F1-score\"]) \n",
    "X.add_row([\"Logistic Regression\", \"58\"]) \n",
    "X.add_row([\"Decision Tree Classifier\", \"70\"])\n",
    "X.add_row([\"Random Forest Classifier\", \"72\"]) \n",
    "X.add_row([\"Xgboost Classifier\", \"73\"])\n",
    "X.add_row([\"Adaboost Classifier\", \"73\"])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Models with Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_inputs1=50\n",
    "encoder1= Input(shape=(num_of_inputs1,))\n",
    "encoding1 = Dense(32)(encoder1)\n",
    "encoding1 = BatchNormalization()(encoding1)\n",
    "encoding1 = LeakyReLU()(encoding1)\n",
    "# encoder level 2\n",
    "encoding1 = Dense(16)(encoding1)\n",
    "encoding1 = BatchNormalization()(encoding1)\n",
    "encoding1 = LeakyReLU()(encoding1)\n",
    "# bottleneck\n",
    "n_bottleneck_features1 =8\n",
    "bottleneck_features1 = Dense(n_bottleneck_features1)(encoding1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decoder, level 1\n",
    "decoding1 = Dense(8)(bottleneck_features1)\n",
    "decoding1 = BatchNormalization()(decoding1)\n",
    "decoding1 = LeakyReLU()(decoding1)\n",
    "# decoder level 2\n",
    "decoding1  = Dense(16)(decoding1)\n",
    "decoding1 = BatchNormalization()(decoding1)\n",
    "decoding1  = LeakyReLU()(decoding1)\n",
    "# output layer\n",
    "decoding1 = Dense(32)(decoding1)\n",
    "Decoder1=Dense(50,activation='linear')(decoding1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define autoencoder model\n",
    "model = Model(inputs=encoder1, outputs=Decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                1650      \n",
      "=================================================================\n",
      "Total params: 4,994\n",
      "Trainable params: 4,850\n",
      "Non-trainable params: 144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',optimizer='Adam',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "291/291 [==============================] - 2s 5ms/step - loss: 0.1685 - val_loss: 0.2197\n",
      "Epoch 2/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0906 - val_loss: 0.2533\n",
      "Epoch 3/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0882 - val_loss: 0.2249\n",
      "Epoch 4/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0777 - val_loss: 0.1632\n",
      "Epoch 5/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0765 - val_loss: 0.2018\n",
      "Epoch 6/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0800 - val_loss: 0.2683\n",
      "Epoch 7/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0726 - val_loss: 0.1800\n",
      "Epoch 8/50\n",
      "291/291 [==============================] - 1s 3ms/step - loss: 0.0694 - val_loss: 0.4063\n",
      "Epoch 9/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0712 - val_loss: 0.5757\n",
      "Epoch 10/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0693 - val_loss: 0.2123\n",
      "Epoch 11/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0661 - val_loss: 0.1948\n",
      "Epoch 12/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0680 - val_loss: 0.3771\n",
      "Epoch 13/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0673 - val_loss: 0.3701\n",
      "Epoch 14/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0689 - val_loss: 0.3145\n",
      "Epoch 15/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0690 - val_loss: 0.1567\n",
      "Epoch 16/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0657 - val_loss: 0.3240\n",
      "Epoch 17/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0710 - val_loss: 0.2161\n",
      "Epoch 18/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0699 - val_loss: 1.1262\n",
      "Epoch 19/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0673 - val_loss: 0.5726\n",
      "Epoch 20/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0673 - val_loss: 0.3092\n",
      "Epoch 21/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0633 - val_loss: 0.2768\n",
      "Epoch 22/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0589 - val_loss: 0.4422\n",
      "Epoch 23/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0656 - val_loss: 0.7213\n",
      "Epoch 24/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0687 - val_loss: 0.1817\n",
      "Epoch 25/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0598 - val_loss: 0.2131\n",
      "Epoch 26/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0631 - val_loss: 0.2832\n",
      "Epoch 27/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0609 - val_loss: 0.2056\n",
      "Epoch 28/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0598 - val_loss: 0.1653\n",
      "Epoch 29/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0584 - val_loss: 0.2073\n",
      "Epoch 30/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0550 - val_loss: 0.1978\n",
      "Epoch 31/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0559 - val_loss: 0.4777\n",
      "Epoch 32/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0579 - val_loss: 0.2053\n",
      "Epoch 33/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0552 - val_loss: 0.3112\n",
      "Epoch 34/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0517 - val_loss: 0.3091\n",
      "Epoch 35/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0570 - val_loss: 0.2200\n",
      "Epoch 36/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0543 - val_loss: 0.3028\n",
      "Epoch 37/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0519 - val_loss: 0.1856\n",
      "Epoch 38/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0527 - val_loss: 0.1990\n",
      "Epoch 39/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0517 - val_loss: 0.1859\n",
      "Epoch 40/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0543 - val_loss: 0.4503\n",
      "Epoch 41/50\n",
      "291/291 [==============================] - 1s 3ms/step - loss: 0.0493 - val_loss: 0.2511\n",
      "Epoch 42/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0494 - val_loss: 0.2652\n",
      "Epoch 43/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0534 - val_loss: 0.1959\n",
      "Epoch 44/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0504 - val_loss: 0.2378\n",
      "Epoch 45/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0528 - val_loss: 0.2987\n",
      "Epoch 46/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0524 - val_loss: 0.1918\n",
      "Epoch 47/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0517 - val_loss: 0.2451\n",
      "Epoch 48/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0612 - val_loss: 0.2184\n",
      "Epoch 49/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0527 - val_loss: 0.4977\n",
      "Epoch 50/50\n",
      "291/291 [==============================] - 1s 4ms/step - loss: 0.0513 - val_loss: 0.2089\n"
     ]
    }
   ],
   "source": [
    "# fit the autoencoder model to reconstruct input\n",
    "Auto_E_D1 = model.fit(x_train, y_train, epochs=50, batch_size=16, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_only = Model(inputs=encoder1, outputs=bottleneck_features1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_only.save('encoder_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "encoder_only = load_model('encoder_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoder = encoder_only.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoder = encoder_only.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Applying models with Auto encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Sgdclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.01, loss='log', random_state=24)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lg_enc = SGDClassifier(alpha=0.01,loss='log',penalty='l2',random_state=24)\n",
    "model_lg_enc.fit(X_train_encoder,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7370291400142146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81      1015\n",
      "           1       0.52      0.64      0.57       392\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.69      0.71      0.69      1407\n",
      "weighted avg       0.76      0.74      0.74      1407\n",
      "\n",
      "[[787 228]\n",
      " [142 250]]\n"
     ]
    }
   ],
   "source": [
    "y_predict_lg_enc = model_lg_enc.predict(X_test_encoder)\n",
    "score_enc = model_lg_enc.score(X_test_encoder, y_test)\n",
    "print(score_enc)\n",
    "print(metrics.classification_report(y_test, y_predict_lg_enc))\n",
    "print(metrics.confusion_matrix(y_test, y_predict_lg_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6921917127855826"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_predict_lg_enc, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=5, max_depth=10 ...............................\n",
      "[CV] ................ min_samples_split=5, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=10 ..............................\n",
      "[CV] ............... min_samples_split=10, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=100, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=100, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=100, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=100, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.1s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=50 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=50, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=1 ..............................\n",
      "[CV] ............... min_samples_split=500, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=1 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=1, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.1s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=500, max_depth=10 .............................\n",
      "[CV] .............. min_samples_split=500, max_depth=10, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=10, max_depth=5 ...............................\n",
      "[CV] ................ min_samples_split=10, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n",
      "[CV] min_samples_split=5, max_depth=5 ................................\n",
      "[CV] ................. min_samples_split=5, max_depth=5, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "                   param_distributions={'max_depth': [1, 5, 10, 50],\n",
       "                                        'min_samples_split': [5, 10, 100, 500]},\n",
       "                   return_train_score=True, verbose=2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_enc = DecisionTreeClassifier()\n",
    "max_depth=[1,5,10,50]\n",
    "min_samples_split=[5,10,100,500]\n",
    "parameters = {'max_depth':max_depth,'min_samples_split':min_samples_split}\n",
    "clf_dt_enc = RandomizedSearchCV(DT_enc, parameters, cv= 10,return_train_score=True,verbose=2)\n",
    "clf_dt_enc.fit(X_train_encoder, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=5, min_samples_split=5)\n"
     ]
    }
   ],
   "source": [
    "print(clf_dt_enc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5, min_samples_split=5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_enc=DecisionTreeClassifier(max_depth=5, min_samples_split=5)\n",
    "clf_enc.fit(X_train_encoder,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736318407960199\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.81      1015\n",
      "           1       0.52      0.69      0.59       392\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.69      0.72      0.70      1407\n",
      "weighted avg       0.77      0.74      0.75      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_dt_enc = clf_enc.predict(X_test_encoder)\n",
    "score_dt = clf_enc.score(X_test_encoder, y_test)\n",
    "print(score_dt)\n",
    "print(metrics.classification_report(y_test, y_predict_dt_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[767 248]\n",
      " [123 269]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict_dt_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6985542648753064"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_predict_dt_enc, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=RandomForestClassifier(n_jobs=-1, random_state=25),\n",
       "                   n_iter=5,\n",
       "                   param_distributions={'max_depth': [3, 5, 10],\n",
       "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000015AF73747F0>,\n",
       "                                        'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000015AF73875E0>,\n",
       "                                        'n_estimators': [100, 200, 500, 1000,\n",
       "                                                         2000]},\n",
       "                   random_state=25, return_train_score=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "param_dist = {'n_estimators':[100,200,500,1000,2000],\n",
    "              'max_depth':[3,5,10],\n",
    "              \"min_samples_split\": sp_randint(10,19),\n",
    "              \"min_samples_leaf\": sp_randint(25,65)}\n",
    "\n",
    "clf_enc = RandomForestClassifier(random_state=25,n_jobs=-1)\n",
    "\n",
    "rf_random_enc = RandomizedSearchCV(clf_enc, param_distributions=param_dist,\n",
    "                                   n_iter=5,cv=5,random_state=25,return_train_score=True)\n",
    "\n",
    "rf_random_enc.fit(X_train_encoder,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=10, min_samples_leaf=33, min_samples_split=14,\n",
      "                       n_estimators=200, n_jobs=-1, random_state=25)\n"
     ]
    }
   ],
   "source": [
    "print(rf_random_enc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_enc = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=33, min_samples_split=14,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
    "            oob_score=False, random_state=25, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_leaf=33, min_samples_split=14,\n",
       "                       n_estimators=200, n_jobs=-1, random_state=25)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_enc.fit(X_train_encoder,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7391613361762616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.74      0.80      1015\n",
      "           1       0.52      0.73      0.61       392\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.70      0.74      0.71      1407\n",
      "weighted avg       0.78      0.74      0.75      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_rf_enc = clf_rf_enc.predict(X_test_encoder)\n",
    "score_rf_enc = clf_rf_enc.score(X_test_encoder, y_test)\n",
    "print(score_rf_enc)\n",
    "print(metrics.classification_report(y_test, y_predict_rf_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[753 262]\n",
      " [105 287]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict_rf_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7070235172565225"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_predict_rf_enc, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   28.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:18:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100,...\n",
       "                                           reg_lambda=None,\n",
       "                                           scale_pos_weight=None,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'colsample_bytree': [0.1, 0.3, 0.5, 1],\n",
       "                                        'learning_rate': [0.01, 0.03, 0.05, 0.1,\n",
       "                                                          0.15, 0.2],\n",
       "                                        'max_depth': [3, 5, 10],\n",
       "                                        'min_child_weight': [1, 4, 7],\n",
       "                                        'n_estimators': [100, 200, 500, 1000,\n",
       "                                                         2000],\n",
       "                                        'subsample': [0.1, 0.3, 0.5, 1]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "model_xgb_enc=XGBClassifier()\n",
    "\n",
    "prams={\n",
    "    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n",
    "     'n_estimators':[100,200,500,1000,2000],\n",
    "     'max_depth':[3,5,10],\n",
    "    'min_child_weight':[1,4,7],\n",
    "    'colsample_bytree':[0.1,0.3,0.5,1],\n",
    "    'subsample':[0.1,0.3,0.5,1]\n",
    "}\n",
    "model_xgb_enc=RandomizedSearchCV(model_xgb_enc,param_distributions=prams,verbose=10,n_jobs=-1,cv=5)\n",
    "model_xgb_enc.fit(X_train_encoder,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 1, 'n_estimators': 100, 'min_child_weight': 4, 'max_depth': 10, 'learning_rate': 0.01, 'colsample_bytree': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print(model_xgb_enc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:18:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=2000, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.5,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb_enc =XGBClassifier(subsample=0.5,n_estimators=2000,min_child_weight=1,max_depth=3,learning_rate=0.01,colsample_bytree=1)\n",
    "model_xgb_enc.fit(X_train_encoder,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738450604122246\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.75      0.81      1015\n",
      "           1       0.52      0.71      0.60       392\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.70      0.73      0.70      1407\n",
      "weighted avg       0.77      0.74      0.75      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_xgb_enc = model_xgb_enc.predict(X_test_encoder)\n",
    "score_xgb_enc = model_xgb_enc.score(X_test_encoder, y_test)\n",
    "print(score_xgb_enc)\n",
    "print(metrics.classification_report(y_test, y_predict_xgb_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[760 255]\n",
      " [113 279]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7038382692096496"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict_xgb_enc))\n",
    "f1_score(y_test, y_predict_xgb_enc, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  45 | elapsed:   17.5s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   21.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=AdaBoostClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.01, 0.03, 0.05],\n",
       "                                        'n_estimators': [100, 200, 500]},\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adaboost_enc=AdaBoostClassifier()\n",
    "\n",
    "parameters={\n",
    "    'learning_rate':[0.01,0.03,0.05],\n",
    "     'n_estimators':[100,200,500]\n",
    "}\n",
    "model_adaboost_enc=RandomizedSearchCV(model_adaboost_enc,param_distributions=parameters,verbose=10,n_jobs=-1,cv=5)\n",
    "model_adaboost_enc.fit(X_train_encoder,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'learning_rate': 0.03}\n"
     ]
    }
   ],
   "source": [
    "print(model_adaboost_enc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.05, n_estimators=500)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adaboost_enc=AdaBoostClassifier(learning_rate=0.05,n_estimators=500)\n",
    "model_adaboost_enc.fit(X_train_encoder,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412935323383084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.76      0.81      1015\n",
      "           1       0.53      0.69      0.60       392\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.70      0.73      0.70      1407\n",
      "weighted avg       0.77      0.74      0.75      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_ada_enc = model_adaboost_enc.predict(X_test_encoder)\n",
    "score_ada_enc= model_adaboost_enc.score(X_test_encoder, y_test)\n",
    "print(score_ada_enc)\n",
    "print(metrics.classification_report(y_test, y_predict_ada_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[771 244]\n",
      " [120 272]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7040715385219871"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_predict_ada_enc))\n",
    "f1_score(y_test, y_predict_ada_enc, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision Table for MOdels with Auto encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparision between various Classification models with auto encoders\n",
      "+--------------------------+----------+\n",
      "|     Classifier Model     | F1-score |\n",
      "+--------------------------+----------+\n",
      "|   Logistic Regression    |    73    |\n",
      "| Decision Tree Classifier |    74    |\n",
      "| Random Forest Classifier |    74    |\n",
      "|    Xgboost Classifier    |    75    |\n",
      "|   Adaboost Classifier    |    74    |\n",
      "+--------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "print('Comparision between various Classification models with auto encoders')\n",
    "X= PrettyTable([\"Classifier Model\",\"F1-score\"]) \n",
    "X.add_row([\"Logistic Regression\", \"73\"]) \n",
    "X.add_row([\"Decision Tree Classifier\", \"74\"])\n",
    "X.add_row([\"Random Forest Classifier\", \"74\"]) \n",
    "X.add_row([\"Xgboost Classifier\", \"75\"])\n",
    "X.add_row([\"Adaboost Classifier\", \"74\"])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision Table between various models with and without auto aencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparision between various Classification models with auto encoders\n",
      "+--------------------------+---------------------------+------------------------+\n",
      "|     Classifier Model     | F1-score without encoders | F1-score with encoders |\n",
      "+--------------------------+---------------------------+------------------------+\n",
      "|   Logistic Regression    |             58            |           73           |\n",
      "| Decision Tree Classifier |             70            |           74           |\n",
      "| Random Forest Classifier |             72            |           74           |\n",
      "|    Xgboost Classifier    |             73            |           75           |\n",
      "|   Adaboost Classifier    |             73            |           74           |\n",
      "+--------------------------+---------------------------+------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "print('Comparision between various Classification models with auto encoders')\n",
    "X= PrettyTable([\"Classifier Model\",\"F1-score without encoders\",\"F1-score with encoders\"]) \n",
    "X.add_row([\"Logistic Regression\", \"58\",\"73\"]) \n",
    "X.add_row([\"Decision Tree Classifier\", \"70\",\"74\"])\n",
    "X.add_row([\"Random Forest Classifier\", \"72\",\"74\"]) \n",
    "X.add_row([\"Xgboost Classifier\", \"73\",\"75\"])\n",
    "X.add_row([\"Adaboost Classifier\", \"73\",\"74\"])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model XGBoost of auto encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "# open a file, where you ant to store the data\n",
    "file = open('flight_rf.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(model_xgb, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = 'final_xg_boost.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_xgb, open(final_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(final_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model = model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7448471926083866"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
